# GLORIA: GeneraLization with chain of thought fOR bIomedical Ai


**BLAH9 Task** 


**Team**

Nina Hosseini-Kivanani, University of Luxembourg, Faculty of Science, Technology and Medicine (FSTM), Department of Computer Science

Dimitra Anastasiou, Luxembourg Institute of Science and Technology (LIST)

Davide Liga, University of Luxembourg, Faculty of Science, Technology and Medicine (FSTM), Department of Computer Science (DCS)



## References

1. **Irvin, J., Rajpurkar, P., Ko, M., Yu, Y., Ciurea-Ilcus, S., Chute, C., Marklund, H., Haghgoo, B., Ball, R., Shpanskaya, K., et al.** (2019). CheXpert: A large chest radiograph dataset with uncertainty labels and expert comparison. *Proceedings of the AAAI Conference on Artificial Intelligence*, 33, 590–597. [Link](https://arxiv.org/abs/1901.07031)

2. **Dogan, R. I., Leaman, R., and Lu, Z.** (2014). NCBI disease corpus: a resource for disease name recognition and concept normalization. *Journal of Biomedical Informatics*, 47, 1–10. [Link](https://doi.org/10.1016/j.jbi.2013.12.006)

3. **Lee, J., Yoon, W., Kim, S., Kim, D., Kim, S., So, C. H., and Kang, J.** (2020). BioBERT: A pre-trained biomedical language representation model for biomedical text mining. *Bioinformatics*, 36(4), 1234–1240. [Link](https://doi.org/10.1093/bioinformatics/btz682)

4. **Li, J., Sun, Y., Johnson, R. J., Sciaky, D., Wei, C.-H., Leaman, R., Davis, A. P., Mattingly, C. J., Wiegers, T. C., and Lu, Z.** (2016). BioCreative V CDR task corpus: a resource for chemical disease relation extraction. *Database*, 2016. [Link](https://doi.org/10.1093/database/baw068)

5. **Nori, H., King, N., McKinney, S. M., Carignan, D., and Horvitz, E.** (2023). Capabilities of GPT-4 on medical challenge problems. [Link](https://arxiv.org/abs/2303.13375)

6. **Wang, Z., Zhao, K., Wang, Z., and Shang, J.** (2022). Formulating few-shot fine-tuning towards language model pre-training: A pilot study on named entity recognition. *Findings of the Association for Computational Linguistics: EMNLP 2022*, 3186–3199. [Link](https://doi.org/10.18653/v1/2022.findings-emnlp.233)

7. **Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al.** (2022). Chain-of-thought prompting elicits reasoning in large language models. *Advances in Neural Information Processing Systems*, 35, 24824–24837. [Link](https://arxiv.org/abs/2201.11903)
